---
title: "7: Data Wrangling"
author: "Environmental Data Analytics | Kateri Salk"
date: "Spring 2019"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

## LESSON OBJECTIVES
1. Describe the usefulness of data wrangling and its place in the data pipeline
2. Wrangle datasets with dplyr functions
3. Apply data wrangling skills to a real-world example dataset

## OPENING DISCUSSION

After we've completed basic data exploration on a dataset, what step comes next? How does this help us to ask and answer questions about datasets?

## SET UP YOUR DATA ANALYSIS SESSION

In assignment 3, you explored the North Temperate Lakes Long-Term Ecological Research Station data for physical and chemical data. What did you learn about this dataset in your assignment?

We will continue working with this dataset today. 

```{r}
#getting raw datasets into format we will use in data analysis: data wrangling; manipulating datasets into form that we want them
getwd()
library(tidyverse)
NTL.phys.data <- read.csv("./Data/Raw/NTL-LTER_Lake_ChemistryPhysics_Raw.csv")

head(NTL.phys.data)
colnames(NTL.phys.data)
summary(NTL.phys.data)
dim(NTL.phys.data)

```

## DATA WRANGLING

Data wrangling takes data exploration one step further: it allows you to process data in ways that are useful for you. An important part of data wrangling is creating tidy datasets, with the following rules: 

1. Each variable has its own column
2. Each observation has its own row
3. Each value has its own cell

What is the best way to wrangle data? There are multiple ways to arrive at a specific outcome in R, and we will illustrate some of those approaches. Your goal should be to write the simplest and most elegant code that will get you to your desired outcome. However, there is sometimes a trade-off of the opportunity cost to learn a new formulation of code and the time it takes to write complex code that you already know. Remember that the best code is one that is easy to understand for yourself and your collaborators. Remember to comment your code, use informative names for variables and functions, and use reproducible methods to arrive at your output.

## WRANGLING IN R: DPLYR

`dplyr` is a package in R that includes functions for data manipulation (i.e., data wrangling or data munging). `dplyr` is included in the tidyverse package, so you should already have it installed on your machine. The functions act as verbs for data wrangling processes. For more information, run this line of code:

```{r, results = "hide"}
vignette("dplyr")
```


### Filter

Filtering allows us to choose certain rows (observations) in our dataset.

A few relevant commands: 
`==`
`!=` #is not equal to
`<`
`<=`
`>`
`>=`
`&`
`|` #or command. The vertical dash underneath the delete button

```{r}
class(NTL.phys.data$lakeid) #categorical variable
class(NTL.phys.data$depth)

# matrix filtering
NTL.phys.data.surface1 <- NTL.phys.data[NTL.phys.data$depth == 0,] #comma means you're selecting rows for matrix subsetting. blank after comma means chosing all columns

# dplyr filtering
NTL.phys.data.surface2 <- filter(NTL.phys.data, depth == 0) 
NTL.phys.data.surface3 <- filter(NTL.phys.data, depth < 0.25)
#new row names get created, matrix subsetting keeps old row names


# Did the methods arrive at the same result?
head(NTL.phys.data.surface1)
dim(NTL.phys.data.surface1)
head(NTL.phys.data.surface2)
dim(NTL.phys.data.surface2)
head(NTL.phys.data.surface3)
dim(NTL.phys.data.surface3)

# Choose multiple conditions to filter
summary(NTL.phys.data$lakename)
NTL.phys.data.PeterPaul1 <- filter(NTL.phys.data, lakename == "Paul Lake" | lakename == "Peter Lake")
#lakename equals "Paul Lake" or lakename equals "Peter Lake".  Don't use "and" because there are no cells that contain both Paul Lake and Paul Lake
NTL.phys.data.PeterPaul2 <- filter(NTL.phys.data, lakename != "Central Long Lake" & 
                                     lakename != "Crampton Lake" & lakename != "East Long Lake" &
                                     lakename != "Hummingbird Lake" & lakename != "Tuesday Lake" &
                                     lakename != "Ward Lake" & lakename != "West Long Lake")
NTL.phys.data.PeterPaul3 <- filter(NTL.phys.data, lakename %in% c("Paul Lake", "Peter Lake"))
# %in% means include 
# use "or" operator to include things, use "and" operator to exclude things

# Choose a range of conditions of a numeric or integer variable
summary(NTL.phys.data$daynum)
NTL.phys.data.JunethruOctober1 <- filter(NTL.phys.data, daynum > 151 & daynum < 305)
NTL.phys.data.JunethruOctober2 <- filter(NTL.phys.data, daynum > 151, daynum < 305) #and or comma will give same result
NTL.phys.data.JunethruOctober3 <- filter(NTL.phys.data, daynum >= 152 & daynum <= 304)
NTL.phys.data.JunethruOctober4 <- filter(NTL.phys.data, daynum %in% c(152:304))
#can make multiple conditions in same line for different columns

# Exercise: 
# filter NTL.phys.data for the year 1999
# what code do you need to use, based on the class of the variable? If you have a factor you have to put it in "", for numerical you do not have to
class(NTL.phys.data$year4)
NTL.phys.data.1999 <- filter(NTL.phys.data, year4 == 1999)

# Exercise: 
# filter NTL.phys.data for Tuesday Lake from 1990 through 1999.
NTL.phys.data.90to99.Tuesday <- filter(NTL.phys.data, year4 >= 1990 & year4 <= 1999, lakename == "Tuesday Lake")

```
Question: Why don't we filter using row numbers?

> ANSWER: For super large datasets, row numbers can get confusing and hard to manage.  They also do not give you a lot of information and you have to manually inspect the rows to find out which ones you need to include/exclude. dplyer constantly renames rows so row assignments will not be consistend as you wrangle. Not as reproducible 

### Arrange

Arranging allows us to change the order of rows in our dataset. By default, the arrange function will arrange rows in ascending order.

```{r}
NTL.phys.data.depth.ascending <- arrange(NTL.phys.data, depth) #default arranges in ascending order
NTL.phys.data.depth.descending <- arrange(NTL.phys.data, desc(depth))

# Exercise: 
# Arrange NTL.phys.data by temperature, in descending order. 
# Which dates, lakes, and depths have the highest temperatures?
NTL.phys.data.temp.descending <- arrange(NTL.phys.data, desc(temperature_C))

#East Long Lake, Hummingbird Lake, West Long Lake, Central Long Lake, Paul Lake, Tuesday Lake
#Dates mostly in June and July
#Depths from 0.0-1.0
```
### Select

Selecting allows us to choose certain columns (variables) in our dataset.

```{r}
NTL.phys.data.temps <- select(NTL.phys.data, lakename, sampledate:temperature_C) #choose lakename to include, and also inlcude all columns between and including sampledate and temperature_C. for select only want to use commas and colons


```
### Mutate

Mutating allows us to add new columns that are functions of existing columns. Operations include addition, subtraction, multiplication, division, log, and other functions.

```{r}

NTL.phys.data.temps2 <- mutate(NTL.phys.data.temps, temperature_F = (temperature_C*9/5) + 32)

```
### Pipes

Sometimes we will want to perform multiple commands on a single dataset on our way to creating a processed dataset. We could do this in a series of subsequent commands or create a function. However, there is another method to do this that looks cleaner and is easier to read. This method is called a pipe. We designate a pipe with `%>%`. A good way to think about the function of a pipe is with the word "then." 

Let's say we want to take our raw dataset (NTL.phys.data), *then* filter the data for Peter and Paul lakes, *then* select temperature and observation information, and *then* add a column for temperature in Fahrenheit: 

```{r}
NTL.phys.data.processed <- 
  NTL.phys.data %>%
  filter(lakename == "Paul Lake" | lakename == "Peter Lake") %>%
  select(lakename, sampledate:temperature_C) %>%
  mutate(temperature_F = (temperature_C*9/5) + 32)

#use raw dataset THEN only include paul and peter lake THEN select lakename, sampledata:temperature_C columns THEN create new column with mutate.
  
```

Notice that we did not place the dataset name inside the wrangling function but rather at the beginning.

### Saving processed datasets

```{r}
write.csv(NTL.phys.data.PeterPaul1, row.names = FALSE, file = "./Data/Processed/NTL-LTER_Lake_ChemistryPhysics_PeterPaul_Processed.csv")

#if you have row.names = TRUE, then the first column in the dataframe will just be a column of numbers
#./ dot means down one into the Data folder
#comment out the write.csv so you don't overwrite it every time you run your code
```

## CLOSING DISCUSSION
How did data wrangling help us to generate a processed dataset? How does this impact our ability to analyze and answer questions about our data?

